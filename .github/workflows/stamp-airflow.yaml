name: '4. Stamp Configure (b)'

on:
  workflow_dispatch:

env:
  CLI_VERSION: 2.30.0
  OSDU_VERSION: 0.11.0
  CLUSTER: osdu-stamp

jobs:

  fileshare-data:
    name: Load Data - File Shares
    runs-on: ubuntu-latest
    steps:
      - uses: Azure/login@v1.1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
      - uses: actions/checkout@v2

      - name: Environment Settings
        uses: Azure/cli@1.0.4
        with:
          azcliversion: ${{ env.CLI_VERSION }}
          inlineScript: |
            GROUP=$(az group list --query "[?contains(name, 'cpl${{ secrets.RAND }}')].name" -otsv)
            ENV_VAULT=$(az keyvault list --resource-group $GROUP --query [].name -otsv)

            echo "GROUP=$GROUP" >> $GITHUB_ENV
            echo "ENV_VAULT=$ENV_VAULT" >> $GITHUB_ENV

      - name: Load Airflow DAGS
        env:
          FILE_SHARE: airflowdags
        uses: Azure/cli@1.0.4
        with:
          azcliversion: ${{ env.CLI_VERSION }}
          inlineScript: |
            GROUP=$(az group list --query "[?contains(name, 'cpl${{ secrets.RAND }}')].name" -otsv)
            ENV_VAULT=$(az keyvault list --resource-group $GROUP --query [].name -otsv)

            az storage file upload-batch \
              --account-name $(az keyvault secret show --id https://${ENV_VAULT}.vault.azure.net/secrets/airflow-storage --query value -otsv) \
              --account-key $(az keyvault secret show --id https://${ENV_VAULT}.vault.azure.net/secrets/airflow-storage-key --query value -otsv) \
              --destination ${{ env.FILE_SHARE }} \
              --source $GITHUB_WORKSPACE/src/${{ env.FILE_SHARE }} \
              --pattern "*.ini"

            az storage file upload-batch \
              --account-name $(az keyvault secret show --id https://${ENV_VAULT}.vault.azure.net/secrets/airflow-storage --query value -otsv) \
              --account-key $(az keyvault secret show --id https://${ENV_VAULT}.vault.azure.net/secrets/airflow-storage-key --query value -otsv) \
              --destination ${{ env.FILE_SHARE }} \
              --source $GITHUB_WORKSPACE/src/${{ env.FILE_SHARE }} \
              --pattern "*.py"

  install-airflow:
    name: Airflow Installation
    env:
      GITHUB_TOKEN: ${{ secrets.GH_REPO_TOKEN }}
    needs: fileshare-data
    runs-on: ubuntu-latest
    steps:
      - uses: Azure/login@v1.1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
      - uses: azure/setup-helm@v1
      - uses: actions/checkout@v2

      - name: Environment Settings
        uses: Azure/cli@1.0.4
        with:
          azcliversion: ${{ env.CLI_VERSION }}
          inlineScript: |
            GROUP=$(az group list --query "[?contains(name, 'dpl${{ secrets.RAND }}')].name" -otsv |grep -v MC)
            ENV_CLUSTER=$(az aks list --resource-group $GROUP --query [].name -otsv)
            echo "CLUSTER_RESOURCE_GROUP=$GROUP" >> $GITHUB_ENV
            echo "CLUSTER_NAME=$ENV_CLUSTER" >> $GITHUB_ENV

      - name: AKS Context
        uses: azure/aks-set-context@v1
        with:
          creds: '${{ secrets.AZURE_CREDENTIALS }}'
          cluster-name: ${{ env.CLUSTER_NAME }}
          resource-group: ${{ env.CLUSTER_RESOURCE_GROUP }}

      - name: Retrieve Stamp Env Variables
        run: |
          mkdir -p $GITHUB_WORKSPACE/clusters/$CLUSTER
          mkdir -p $GITHUB_WORKSPACE/apps/$CLUSTER/

          GROUP=$(az group list --query "[?contains(name, 'cpl${{ secrets.RAND }}')].name" -otsv)
          ENV_VAULT=$(az keyvault list --resource-group $GROUP --query [].name -otsv)
          REGISTRY=$(az acr list -g $GROUP --query [0].loginServer -otsv)
          DP_GROUP=$(az group list --query "[?contains(name, 'dpl${{ secrets.RAND }}')].name" -otsv |grep -v MC)
          DNS_HOST=$(az network public-ip list --resource-group $DP_GROUP --query [].dnsSettings.fqdn -otsv)
          POSTGRES_HOST=$(az keyvault secret show --id https://${ENV_VAULT}.vault.azure.net/secrets/base-name-sr --query value -otsv)-pg.postgres.database.azure.com
          POSTGRES_USER=osdu_admin@$(az keyvault secret show --id https://${ENV_VAULT}.vault.azure.net/secrets/base-name-sr --query value -otsv)-pg
          REDIS_HOST=$(az keyvault secret show --id https://${ENV_VAULT}.vault.azure.net/secrets/base-name-sr --query value -otsv)-cache.redis.cache.windows.net

          echo "GROUP=$GROUP" >> $GITHUB_ENV
          echo "ENV_VAULT=$ENV_VAULT" >> $GITHUB_ENV
          echo "REGISTRY=$REGISTRY" >> $GITHUB_ENV
          echo "DP_GROUP=$DP_GROUP" >> $GITHUB_ENV
          echo "DNS_HOST=$DNS_HOST" >> $GITHUB_ENV
          echo "POSTGRES_HOST=$POSTGRES_HOST" >> $GITHUB_ENV
          echo "POSTGRES_USER=$POSTGRES_USER" >> $GITHUB_ENV
          echo "REDIS_HOST=$REDIS_HOST" >> $GITHUB_ENV

      - name: Helm Values
        run: |
          cat > ${{ runner.temp }}/custom-values.yaml << EOF
          airflowLogin:
            name: admin
          ingress:
            enabled: true
            web:
              annotations:
                kubernetes.io/ingress.class: azure/application-gateway
                appgw.ingress.kubernetes.io/request-timeout: "300"
                appgw.ingress.kubernetes.io/connection-draining: "true"
                appgw.ingress.kubernetes.io/connection-draining-timeout: "30"
                cert-manager.io/cluster-issuer: letsencrypt-prod-dns
                cert-manager.io/acme-challenge-type: http01
              path: "/airflow"
              host: $DNS_HOST
              livenessPath: "/airflow/health"
              tls:
                enabled: true
                secretName: osdu-certificate
              precedingPaths:
                - path: "/airflow/*"
                  serviceName: airflow-web
                  servicePort: 8080
          postgresql:
            enabled: false
          externalDatabase:
            type: postgres
            host: $POSTGRES_HOST
            user: $POSTGRES_USER
            passwordSecret: "postgres"
            passwordSecretKey: "postgres-password"
            port: 5432
            properties: "?sslmode=require"
            database: airflow
          redis:
            enabled: false
          externalRedis:
            host: $REDIS_HOST
            passwordSecret: "redis"
            passwordSecretKey: "redis-password"
            port: 6380
          dags:
            installRequirements: true
            persistence:
              enabled: true
              existingClaim: airflowdag-pvc
              subPath: "dags"
          web:
            podLabels:
              aadpodidbinding: "osdu-identity"
            baseUrl: "http://localhost/airflow"
          workers:
            podLabels:
              aadpodidbinding: "osdu-identity"
            replicas: 2
          flower:
            enabled: false
          scheduler:
            podLabels:
              aadpodidbinding: "osdu-identity"
            variables: |
              {}
          airflow:
            image:
              repository: apache/airflow
              tag: 1.10.12-python3.6
              pullPolicy: IfNotPresent
              pullSecret: ""
            extraVolumeMounts:
              - name: azure-keyvault
                mountPath: "/mnt/azure-keyvault"
                readOnly: true
              - name: dags-data
                mountPath: /opt/airflow/plugins
                subPath: plugins
            extraConfigmapMounts:
              - name: remote-log-config
                mountPath: /opt/airflow/config
                configMap: airflow-remote-log-config
                readOnly: true
            extraVolumes:
              - name: azure-keyvault
                csi:
                  driver: secrets-store.csi.k8s.io
                  readOnly: true
                  volumeAttributes:
                    secretProviderClass: azure-keyvault
            extraPipPackages: [
              "flask-bcrypt==0.7.1",
              "apache-airflow[statsd]",
              "apache-airflow[kubernetes]",
              "apache-airflow-backport-providers-microsoft-azure==2021.2.5",
              "dataclasses==0.8",
              "google-cloud-storage",
              "python-keycloak==0.24.0",
              "msal==1.9.0",
              "azure-identity==1.5.0",
              "azure-keyvault-secrets==4.2.0",
              "azure-storage-blob",
              "azure-servicebus==7.0.1",
              "toposort==1.6",
              "strict-rfc3339==0.7",
              "jsonschema==3.2.0",
              "pyyaml==5.4.1",
              "requests==2.25.1",
              "tenacity==8.0.1",
              "https://azglobalosdutestlake.blob.core.windows.net/pythonsdk/osdu_api-0.11.0-e908cdaa.tar.gz",
              "https://azglobalosdutestlake.blob.core.windows.net/pythonsdk/osdu_airflow-0.0.1.tar.gz"
            ]
            config:
              AIRFLOW__CORE__LOGGING_LEVEL: DEBUG
              AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "False"
              AIRFLOW__CORE__PLUGINS_FOLDER: "/opt/airflow/plugins"
              AIRFLOW__CELERY__SSL_ACTIVE: "True"
              AIRFLOW__SCHEDULER__STATSD_ON: "False"
              AIRFLOW__SCHEDULER__STATSD_HOST: "appinsights-statsd"
              AIRFLOW__SCHEDULER__STATSD_PORT: 8125
              AIRFLOW__SCHEDULER__STATSD_PREFIX: "osdu_airflow"
              AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 60
              AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
              AIRFLOW__WEBSERVER__RBAC: "True"
              AIRFLOW__WEBSERVER__AUTHENTICATE: "True"
              AIRFLOW__WEBSERVER__AUTH_BACKEND: "airflow.contrib.auth.backends.password_auth"
              AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: "True"
              AIRFLOW__API__AUTH_BACKEND: "airflow.contrib.auth.backends.password_auth"
              AIRFLOW__CORE__REMOTE_LOGGING: "True"
              AIRFLOW__CORE__REMOTE_LOG_CONN_ID: "az_log"
              AIRFLOW__CORE__REMOTE_BASE_LOG_FOLDER: "wasb-airflowlog"
              AIRFLOW__CORE__LOGGING_CONFIG_CLASS: "log_config.DEFAULT_LOGGING_CONFIG"
              AIRFLOW__CORE__LOG_FILENAME_TEMPLATE: "{{ run_id }}/{{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{% if dag_run.conf is not none and 'correlation_id' in dag_run.conf %}{{ dag_run.conf['correlation_id'] }}{% else %}None{% endif %}/{{ try_number }}.log"
            extraEnv:
            - name: AIRFLOW_VAR_AZURE_DNS_HOST
              value: "$DNS_HOST"
            - name: CLOUD_PROVIDER
              value: "azure"
            - name: AIRFLOW_VAR_AZURE_ENABLE_MSI
              value: "false"
            - name: CI_COMMIT_TAG
              value: "v0.11.0"
            - name: BUILD_TAG
              value: "v0.11.0"
            - name: AIRFLOW_VAR_KEYVAULT_URI
              valueFrom:
                configMapKeyRef:
                  name: osdu-svc-config
                  key: ENV_KEYVAULT
            - name: AIRFLOW__CORE__FERNET_KEY
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: fernet-key
            - name: AIRFLOW_CONN_AZ_LOG
              valueFrom:
                secretKeyRef:
                  name: airflow
                  key: remote-log-connection
            - name: AIRFLOW_VAR_AZURE_TENANT_ID
              valueFrom:
                secretKeyRef:
                  name: active-directory
                  key: tenantid
            - name: AIRFLOW_VAR_AZURE_CLIENT_ID
              valueFrom:
                secretKeyRef:
                  name: active-directory
                  key: principal-clientid
            - name: AIRFLOW_VAR_AZURE_CLIENT_SECRET
              valueFrom:
                secretKeyRef:
                  name: active-directory
                  key: principal-clientpassword
            - name: AIRFLOW_VAR_AAD_CLIENT_ID
              valueFrom:
                secretKeyRef:
                  name: active-directory
                  key: application-appid
            - name: AIRFLOW_VAR_APPINSIGHTS_KEY
              valueFrom:
                secretKeyRef:
                  name: central-logging
                  key: appinsights
            - name: AIRFLOW_VAR_ENTITLEMENTS_MODULE_NAME
              value: "entitlements_client"
            - name: AIRFLOW_VAR_CORE__CONFIG__DATALOAD_CONFIG_PATH
              value: "/opt/airflow/dags/configs/dataload.ini"
            - name: AIRFLOW_VAR_CORE__SERVICE__SCHEMA__URL
              value:  "http://schema.osdu-azure.svc.cluster.local/api/schema-service/v1/schema"
            - name: AIRFLOW_VAR_CORE__SERVICE__SEARCH__URL
              value: "http://search.osdu-azure.svc.cluster.local/api/search/v2/query"
            - name: AIRFLOW_VAR_CORE__SERVICE__STORAGE__URL
              value:  "http://storage.osdu-azure.svc.cluster.local/api/storage/v2/records"
            - name: AIRFLOW_VAR_CORE__SERVICE__FILE__HOST
              value: "http://file.osdu-azure.svc.cluster.local/api/file/v2"
            - name: AIRFLOW_VAR_CORE__SERVICE__WORKFLOW__HOST
              value: "http://workflow.osdu-azure.svc.cluster.local/api/workflow"
            - name: AIRFLOW_VAR_CORE__SERVICE__SEARCH_WITH_CURSOR__URL
              value: "http://search.osdu-azure.svc.cluster.local/api/search/v2/query_with_cursor"
          EOF

      - name: Helm Install
        run: |
          helm upgrade --install airflow $GITHUB_WORKSPACE/charts/airflow --values ${{ runner.temp }}/custom-values.yaml --namespace airflow

      - name: User Job - Create
        run: |
          cat > ${{ runner.temp }}/create-user.yaml << EOF
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: "create-user"
          spec:
            ttlSecondsAfterFinished: 60
            template:
              metadata:
                name: "create-user"
                labels:
                  aadpodidbinding: "osdu-identity"
              spec:
                restartPolicy: Never
                containers:
                - name: post-install-job
                  image: python:3.6.12-slim-buster
                  command: ['sh','-c','pip install --user -r /scripts/requirements.txt && python /scripts/create_default_user.py']
                  volumeMounts:
                  - name: config-volume
                    mountPath: /scripts
                  - name: azure-keyvault
                    mountPath: "/mnt/azure-keyvault"
                    readOnly: true
                  env:
                  - name: DATABASE_USER
                    value: $POSTGRES_USER
                  - name: DATABASE_HOST
                    value: $POSTGRES_HOST
                  - name: DATABASE_DB
                    value: airflow
                  - name: DATABASE_USER_PASS
                    valueFrom:
                      secretKeyRef:
                        name: postgres
                        key: postgres-password
                  - name: AIRFLOW_ADMIN
                    value: admin
                  - name: AIRFLOW_ADMIN_PASS
                    valueFrom:
                      secretKeyRef:
                        name: airflow
                        key: admin-password
                volumes:
                - name: config-volume
                  configMap:
                    name: post-install-job-config
                - name: azure-keyvault
                  csi:
                    driver: secrets-store.csi.k8s.io
                    readOnly: true
                    volumeAttributes:
                      secretProviderClass: azure-keyvault
          EOF
          kubectl apply -f ${{ runner.temp }}/create-user.yaml --namespace airflow
      - name: User Job - Cleanup
        run: |
          while true; do
            if kubectl wait --for=condition=complete --timeout=0 job/create-user --namespace airflow 2>/dev/null; then
              job_result=0
              break
            fi
            if kubectl wait --for=condition=failed --timeout=0 job/create-user --namespace airflow 2>/dev/null; then
              job_result=1
              break
            fi
            sleep 3
          done
          if [[ $job_result -eq 1 ]]; then
              echo "Job failed!"
              exit 1
          fi
          echo "Job succeeded"
          POD=$(kubectl get pods --selector=job-name=create-user --output=jsonpath='{.items[*].metadata.name}' --namespace airflow)
          if [ ! -z "$POD" ]
          then
              kubectl delete pod $POD --namespace airflow
              echo "Pod Removed"
          fi
